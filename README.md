# NLP Course, 2026

Current curriculum covers topic from basic NLP techinques to the most modern ones, that may be helpful for custom training of LLMs:

- **NLP Basics**: tokenization, text preprocessing, text representations
- **Text & Language Models**: embeddings, n-gram models, RNNs, LSTMs, seq2seq, attention
- **Transformers & LLMs**: Transformer, pre-training (MLM/CLM), prompting, fine-tuning, PEFT
- **Scaling & Optimization**: : distributed training, MoE, efficient inference, quantization
- **Retrieval & Agents:** Information Retrieval, RAG, agent-based systems
- **Post-training**: alignment, RLHF, DPO

## Course Staff

- German Gritsai [@grgera](https://github.com/grgera)
- Anastasiia Vozniuk [@natriistorm](https://github.com/natriistorm)
- Ildar Khabutdinov [@depinwhite](https://github.com/depinwhite)

## Materials

| Week # | Date | Topic | Lecture | Seminar | Recording |
| :-: | - | - | :-: | :-: | :-: |
| 1 | February 10 |Intro to NLP | [slides](slides/week1_nlp.pdf), | [ipynb]() | [record]() |


## Homeworks

TBA

## Game Rules

Final mark = 0.3 Oral Exam + 0.7 Homework

Both oral exam and homeworks are blocking parts, you need to pass both parts to pass the course.

## Prerequisities
- Probability Theory + Statistics
- Machine Learning
- Python [Python guide](http://lxmls.it.pt/2025/documents/LxMLS_guide_2025.pdf)
- Basic knowledge on NLP

We expect students to know basics of Natural Language Processing, as the course focuses on more advanced topics. When you unsure about the basics, we recommned to read these lectures / materials:
1. [Course from Lena Voita](https://lena-voita.github.io/nlp_course.html)
2. [Speech and Language Processing by Jurafsky and Martin](https://web.stanford.edu/~jurafsky/slp3/)
4. [Stanford CS 224n](https://web.stanford.edu/class/cs224n/)
5. [Great blog on Transformer & BERT](http://jalammar.github.io)
