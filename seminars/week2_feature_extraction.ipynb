{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "90c916d9",
      "metadata": {
        "id": "90c916d9"
      },
      "source": [
        "##Count-based вектора: Bag-of-Words и TF-IDF\n",
        "\n",
        "**Bag-of-Words (BoW)**: считаем, сколько раз токены встречаются в документе.  \n",
        "**TF-IDF**: downweight частые слова, upweight специфические.\n",
        "\n",
        "Плюсы:\n",
        "- быстро, просто, часто работает “удивительно хорошо” на классификации.\n",
        "\n",
        "Минусы:\n",
        "- теряется порядок слов (мешок слов)\n",
        "- огромная разреженность\n",
        "- слабая переносимость на новые домены\n",
        "- плохо с семантикой (синонимы, перефразирование)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "cbf4fc54",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbf4fc54",
        "outputId": "bd4e6804-17e5-49d8-816f-882a01dfeae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoW shape: (6, 24) nnz: 35\n",
            "TF-IDF shape: (6, 24) nnz: 35\n",
            "\n",
            "Vocabulary: ['love', 'this', 'movie', 'it', 'is', 'fantastic', 'and', 'fun', 'film', 'was'] ...\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "toy_docs = [\n",
        "    \"I love this movie, it is fantastic and fun\",\n",
        "    \"This film was terrible, boring and bad\",\n",
        "    \"What a great and wonderful experience\",\n",
        "    \"Awful plot, bad acting, I hate it\",\n",
        "    \"Fantastic acting and great story\",\n",
        "    \"Boring movie, not good\",\n",
        "]\n",
        "toy_y = np.array([1,0,1,0,1,0])  # 1=positive, 0=negative\n",
        "\n",
        "bow = CountVectorizer(lowercase=True)\n",
        "X_bow = bow.fit_transform(toy_docs)\n",
        "\n",
        "tfidf = TfidfVectorizer(lowercase=True)\n",
        "X_tfidf = tfidf.fit_transform(toy_docs)\n",
        "\n",
        "print(\"BoW shape:\", X_bow.shape, \"nnz:\", X_bow.nnz)\n",
        "print(\"TF-IDF shape:\", X_tfidf.shape, \"nnz:\", X_tfidf.nnz)\n",
        "print(\"\\nVocabulary:\", list(bow.vocabulary_.keys())[:10], \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4b47369f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b47369f",
        "outputId": "dbb30043-144e-4fd9-d3ba-e9bdf012461a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('is', np.float64(0.4068386546370098)),\n",
              " ('fun', np.float64(0.4068386546370098)),\n",
              " ('love', np.float64(0.4068386546370098)),\n",
              " ('this', np.float64(0.3336135167099814)),\n",
              " ('movie', np.float64(0.3336135167099814)),\n",
              " ('it', np.float64(0.3336135167099814)),\n",
              " ('fantastic', np.float64(0.3336135167099814)),\n",
              " ('and', np.float64(0.24136075313322858))]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Посмотрим топ-слова по TF-IDF в одном документе\n",
        "doc_id = 0\n",
        "row = X_tfidf[doc_id].toarray().ravel()\n",
        "top = row.argsort()[::-1][:8]\n",
        "inv_vocab = {i:w for w,i in tfidf.vocabulary_.items()}\n",
        "[(inv_vocab[i], row[i]) for i in top if row[i] > 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1f6305e",
      "metadata": {
        "id": "c1f6305e"
      },
      "source": [
        "### Ограничения count-based подходов (что проговорить)\n",
        "\n",
        "1) **Нет порядка**: “dog bites man” vs “man bites dog” → одинаково (в BoW).  \n",
        "2) **Синонимы**: “great” и “excellent” — разные координаты.  \n",
        "3) **Разреженность**: словарь растёт, память/время растут.  \n",
        "4) **Доменные сдвиги**: новые слова/жанры → деградация.  \n",
        "5) **Морфология**: без лемматизации “кошки/кошку/кошкой” раздувают словарь."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff39feaf",
      "metadata": {
        "id": "ff39feaf"
      },
      "source": [
        "## Эмбеддинги: Word2Vec, GloVe, “плотные” представления (7–9 мин)\n",
        "\n",
        "**Эмбеддинг**: отображение токена в плотный вектор `R^d`, где близость ≈ семантическая близость.\n",
        "\n",
        "- **Word2Vec (CBOW/Skip-gram)**: предсказываем контекст по слову или слово по контексту.\n",
        "- **GloVe**: использует глобальные матрицы совместной встречаемости (co-occurrence) и факторизацию с весами."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQD-qvIff-aY",
        "outputId": "497c701f-b854-4e0c-f92b-199abb24bc25"
      },
      "id": "yQD-qvIff-aY",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.1.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "3a02689e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a02689e",
        "outputId": "9be40418-932d-470f-b776-a6089998d28f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('boring', 0.23871083557605743), ('this', 0.19837909936904907), ('film', 0.19324013590812683), ('good', 0.15575988590717316), ('i', 0.15475760400295258)]\n"
          ]
        }
      ],
      "source": [
        "def simple_word_tokenize(text):\n",
        "    return text.split()\n",
        "\n",
        "\n",
        "def try_train_word2vec(sentences):\n",
        "    try:\n",
        "        from gensim.models import Word2Vec\n",
        "        model = Word2Vec(sentences=sentences, vector_size=50, window=3, min_count=1, sg=1, epochs=200, workers=1)\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(\"gensim Word2Vec not available:\", type(e).__name__, \"-\", e)\n",
        "        return None\n",
        "\n",
        "sentences = [simple_word_tokenize(t.lower()) for t in toy_docs]\n",
        "w2v = try_train_word2vec(sentences)\n",
        "\n",
        "if w2v is not None:\n",
        "    print(w2v.wv.most_similar(\"fun\", topn=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5403aaa8",
      "metadata": {
        "id": "5403aaa8"
      },
      "source": [
        "## Базовая классификация текста\n",
        "Собираем простой pipeline:\n",
        "- текст → TF-IDF\n",
        "- классификатор: Logistic Regression / Linear SVM / Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "categories = ['sci.space', 'rec.sport.hockey']\n",
        "\n",
        "data = fetch_20newsgroups(\n",
        "    subset='all',\n",
        "    categories=categories,\n",
        "    remove=('headers', 'footers', 'quotes')\n",
        ")\n",
        "\n",
        "for class_id, class_name in enumerate(data.target_names):\n",
        "    idx = list(data.target).index(class_id)\n",
        "    print(\"=\"*80)\n",
        "    print(\"CLASS:\", class_name)\n",
        "    print(\"=\"*80)\n",
        "    print(data.data[idx][:1000])\n",
        "    print(\"\\n\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAImlOdHg1E2",
        "outputId": "53e3cac1-c49b-4f71-ac85-a9888bcddc2f"
      },
      "id": "QAImlOdHg1E2",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "CLASS: rec.sport.hockey\n",
            "================================================================================\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\tI think that you are incorrect, Roger.  Patrick,\n",
            "Smythe and Adams all played or coached in the league before becoming\n",
            "front office types.  Hence, they did help build the league, although\n",
            "they were not great players themselves.  \n",
            "\n",
            "\tI agree that a name is a name is a name, and if some people\n",
            "have trouble with names that are not easily processed by the fans,\n",
            "then changing them to names that are more easily processed seems like\n",
            "a reasonable idea.  If we can get people in the (arena) door by being\n",
            "uncomplicated, then let's do so.  Once we have them, they will realize\n",
            "what a great game hockey is, and we can then teach them something\n",
            "abotu the history of the game.  \n",
            " \n",
            "\n",
            "\tNo, I would not want to see a Ballard division.  But to say\n",
            "that these owners are assholes, hence all NHL management people are\n",
            "assholes would be fallacious.  Conn Smythe, for example, was a classy\n",
            "individual (from what I have heard). \n",
            "\n",
            "\tAlso, isn't the point of \"professional\" hockey to make money\n",
            "for all those involved, \n",
            "\n",
            "\n",
            "\n",
            "================================================================================\n",
            "CLASS: sci.space\n",
            "================================================================================\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Well, I guess I'm left wondering just who all the 'light fascists'\n",
            "think *they* are.  Yes, I understand the issues.  I don't even\n",
            "particularly care for the idea.  But am I the only one that finds the\n",
            "sort of overreaction above just a *little* questionable?  You must\n",
            "find things like the Moon *really* obnoxious in their pollution.\n",
            "\n",
            "A few questions for those frothing at the mouth to ask themselves:\n",
            "\n",
            "\t1) How long is this thing supposed to stay up?  Sounds like it\n",
            "would have a *huge* drag area, not a lot of mass, and be in a fairly\n",
            "low orbit.\n",
            "\n",
            "\t2) Just what orbital parameters are we talking about here?\n",
            "What real impact are we talking about, really?  How many optical\n",
            "astronomers are *really* going to be impacted?\n",
            "\n",
            "\t3) Which is more important; adding a few extra days of\n",
            "'seeing' for (very few) optical astronomers or getting the data the\n",
            "sensors are supposed to return along with the data for large\n",
            "inflatables (and the potential there for an inflatable space station)?\n",
            "The choice would se\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "878d377a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "878d377a",
        "outputId": "cd95ca71-bb89-485b-ba09-424812992d0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 1986\n",
            "Class names: ['rec.sport.hockey', 'sci.space']\n",
            "\n",
            "Classification report:\n",
            "\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "rec.sport.hockey       0.99      0.93      0.96       300\n",
            "       sci.space       0.93      0.99      0.96       296\n",
            "\n",
            "        accuracy                           0.96       596\n",
            "       macro avg       0.96      0.96      0.96       596\n",
            "    weighted avg       0.96      0.96      0.96       596\n",
            "\n",
            "Confusion matrix:\n",
            " [[278  22]\n",
            " [  4 292]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "categories = ['sci.space', 'rec.sport.hockey']\n",
        "\n",
        "data = fetch_20newsgroups(\n",
        "    subset='all',\n",
        "    categories=categories,\n",
        "    remove=('headers', 'footers', 'quotes')\n",
        ")\n",
        "\n",
        "X = data.data\n",
        "y = data.target  # 0 или 1\n",
        "\n",
        "print(\"Total samples:\", len(X))\n",
        "print(\"Class names:\", data.target_names)\n",
        "print()\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.3,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "# Pipeline: TF-IDF + Logistic Regression\n",
        "clf = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer(lowercase=True, stop_words=\"english\", ngram_range=(1,2))),\n",
        "    (\"lr\", LogisticRegression(max_iter=2000))\n",
        "])\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "pred = clf.predict(X_test)\n",
        "\n",
        "print(\"Classification report:\\n\")\n",
        "print(classification_report(y_test, pred, target_names=data.target_names))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b642ee3e",
      "metadata": {
        "id": "b642ee3e"
      },
      "source": [
        "### Интерпретация: какие признаки важны?\n",
        "\n",
        "Для линейных моделей на TF-IDF можно посмотреть веса по словам/нграммам."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "5639e4ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5639e4ae",
        "outputId": "4dd5a830-22b0-4ca0-98f4-4ba7715990ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top positive features:\n",
            "space                3.534\n",
            "orbit                1.490\n",
            "nasa                 1.481\n",
            "shuttle              1.302\n",
            "launch               1.298\n",
            "earth                1.293\n",
            "moon                 1.230\n",
            "program              1.055\n",
            "sky                  1.050\n",
            "spacecraft           1.049\n",
            "\n",
            "Top negative features:\n",
            "game                 -3.524\n",
            "hockey               -2.637\n",
            "team                 -2.437\n",
            "games                -2.385\n",
            "season               -1.722\n",
            "players              -1.522\n",
            "play                 -1.475\n",
            "espn                 -1.426\n",
            "nhl                  -1.358\n",
            "player               -1.218\n"
          ]
        }
      ],
      "source": [
        "vec = clf.named_steps[\"tfidf\"]\n",
        "lr = clf.named_steps[\"lr\"]\n",
        "\n",
        "feature_names = np.array(vec.get_feature_names_out())\n",
        "coef = lr.coef_[0]  # positive class\n",
        "top_pos = coef.argsort()[::-1][:10]\n",
        "top_neg = coef.argsort()[:10]\n",
        "\n",
        "print(\"Top positive features:\")\n",
        "for i in top_pos:\n",
        "    print(f\"{feature_names[i]:<20} {coef[i]:.3f}\")\n",
        "\n",
        "print(\"\\nTop negative features:\")\n",
        "for i in top_neg:\n",
        "    print(f\"{feature_names[i]:<20} {coef[i]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zQTOZY_ChpxQ"
      },
      "id": "zQTOZY_ChpxQ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}