{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0c9223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487c1d87",
   "metadata": {},
   "source": [
    "В реальном проекте корпус — тысячи/миллионы строк. Здесь сделаем маленький корпус, чтобы все выполнялось мгновенно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ccaa9d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP is fun! Fun, fun, fun...',\n",
       " 'I love machine learning and natural language processing.',\n",
       " 'Tokenization splits text into pieces — tokens.',\n",
       " \"Cats, cat's, and catlike: morphology matters.\",\n",
       " 'Я люблю NLP и обработку естественного языка.',\n",
       " 'Кошки любят молоко, а кот любит рыбу.',\n",
       " 'Привет!   Привет!! Привет???',\n",
       " 'é vs é  — Unicode normalization example.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"NLP is fun! Fun, fun, fun...\",\n",
    "    \"I love machine learning and natural language processing.\",\n",
    "    \"Tokenization splits text into pieces — tokens.\",\n",
    "    \"Cats, cat's, and catlike: morphology matters.\",\n",
    "    \"Я люблю NLP и обработку естественного языка.\",\n",
    "    \"Кошки любят молоко, а кот любит рыбу.\",\n",
    "    \"Привет!   Привет!! Привет???\",\n",
    "    \"é vs é  — Unicode normalization example.\",\n",
    "]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6786daf2",
   "metadata": {},
   "source": [
    "**Идея:** модель не понимает строки → мы превращаем текст в *последовательность дискретных единиц* (токенов), а затем в числа.\n",
    "\n",
    "Типичные шаги:\n",
    "- **Unicode-нормализация** (NFC/NFKC)\n",
    "- нормализация пробелов\n",
    "- приведение к lower (не всегда!)\n",
    "- обработка чисел/URL/емодзи (зависит от задачи)\n",
    "- лемматизация/стемминг (опционально, зависит от модели и языка)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77669deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW:  'Привет!   Привет!! Привет???'\n",
      "NFC:  'Привет! Привет!! Привет???'\n",
      "\n",
      "RAW:  'é vs é  — Unicode normalization example.'\n",
      "NFC:  'é vs é — Unicode normalization example.'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def normalize_unicode(text: str, form: str = \"NFC\") -> str:\n",
    "    return unicodedata.normalize(form, text)\n",
    "\n",
    "def normalize_spaces(text: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "def basic_preprocess(text: str) -> str:\n",
    "    text = normalize_unicode(text, \"NFC\")\n",
    "    text = normalize_spaces(text)\n",
    "    return text\n",
    "\n",
    "for t in corpus[-2:]:\n",
    "    print(\"RAW: \", repr(t))\n",
    "    print(\"NFC: \", repr(basic_preprocess(t)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba735358-cff0-44cd-913a-d6bfeacf9385",
   "metadata": {},
   "source": [
    "## Стемминг VS Лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5451eec",
   "metadata": {},
   "source": [
    "**Нормализация**: привести разные варианты записи к одному виду (Unicode, ё/е, кавычки, пробелы).\n",
    "\n",
    "**Лемматизация**: привести слово к “словарной форме”:\n",
    "- *cats → cat*\n",
    "- *кошки → кошка*\n",
    "\n",
    "Зачем:\n",
    "- уменьшить разреженность в count-based признаках\n",
    "- лучше обобщать на формы слова\n",
    "\n",
    "Но:\n",
    "- для современных subword-токенизаторов/LLM лемматизация часто **не нужна** и иногда вредна (ломает стиль/смысл)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec12d423-6cdc-486b-8875-6eb0f086e3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming (Porter):\n",
      "running    -> run\n",
      "runner     -> runner\n",
      "runs       -> run\n",
      "better     -> better\n",
      "cars       -> car\n",
      "studies    -> studi\n",
      "studying   -> studi\n",
      "wolves     -> wolv\n",
      "went       -> went\n",
      "\n",
      "Lemmatization (WordNet) WITHOUT POS (default noun):\n",
      "running    -> running\n",
      "runner     -> runner\n",
      "runs       -> run\n",
      "better     -> better\n",
      "cars       -> car\n",
      "studies    -> study\n",
      "studying   -> studying\n",
      "wolves     -> wolf\n",
      "went       -> went\n",
      "\n",
      "Lemmatization (WordNet) WITH POS (better):\n",
      "running    (VBG ) -> run\n",
      "runner     (NN  ) -> runner\n",
      "runs       (VBZ ) -> run\n",
      "better     (JJR ) -> good\n",
      "cars       (NNS ) -> car\n",
      "studies    (NNS ) -> study\n",
      "studying   (VBG ) -> study\n",
      "wolves     (NNS ) -> wolf\n",
      "went       (VBD ) -> go\n"
     ]
    }
   ],
   "source": [
    "# Examples: stemming vs lemmatization (English)\n",
    "\n",
    "# pip install nltk spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "\n",
    "# -------------------- NLTK: stemming --------------------\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# If you haven't downloaded NLTK resources yet:\n",
    "# nltk.download(\"wordnet\")\n",
    "# nltk.download(\"omw-1.4\")\n",
    "# nltk.download(\"averaged_perceptron_tagger_eng\")  # new tagger name in recent NLTK\n",
    "# nltk.download(\"averaged_perceptron_tagger\")      # fallback for older NLTK\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = [\"running\", \"runner\", \"runs\", \"better\", \"cars\", \"studies\", \"studying\", \"wolves\", \"went\"]\n",
    "print(\"Stemming (Porter):\")\n",
    "for w in words:\n",
    "    print(f\"{w:10s} -> {stemmer.stem(w)}\")\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def to_wordnet_pos(treebank_tag: str):\n",
    "    \"\"\"Map Penn Treebank POS tags to WordNet POS tags (rough but useful).\"\"\"\n",
    "    if treebank_tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    if treebank_tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    if treebank_tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    if treebank_tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    return wordnet.NOUN  # default\n",
    "\n",
    "print(\"\\nLemmatization (WordNet) WITHOUT POS (default noun):\")\n",
    "for w in words:\n",
    "    print(f\"{w:10s} -> {lemmatizer.lemmatize(w)}\")\n",
    "\n",
    "print(\"\\nLemmatization (WordNet) WITH POS (better):\")\n",
    "tagged = nltk.pos_tag(words)\n",
    "for w, tag in tagged:\n",
    "    wn_pos = to_wordnet_pos(tag)\n",
    "    print(f\"{w:10s} ({tag:4s}) -> {lemmatizer.lemmatize(w, pos=wn_pos)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aabd3e1-4879-44cd-a51c-22eb8c597002",
   "metadata": {},
   "source": [
    "### Простая токенизация “по словам” (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ff0c9a5-1644-4527-996c-7fba24b42bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP is fun! Fun, fun, fun...\n",
      "['NLP', 'is', 'fun', '!', 'Fun', ',', 'fun', ',', 'fun', '.', '.', '.']\n",
      "\n",
      "I love machine learning and natural language processing.\n",
      "['I', 'love', 'machine', 'learning', 'and', 'natural', 'language', 'processing', '.']\n",
      "\n",
      "Tokenization splits text into pieces — tokens.\n",
      "['Tokenization', 'splits', 'text', 'into', 'pieces', '—', 'tokens', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_re = re.compile(r\"[\\w']+|[^\\w\\s]\", re.UNICODE)\n",
    "\n",
    "def simple_word_tokenize(text: str):\n",
    "    text = basic_preprocess(text)\n",
    "    return token_re.findall(text)\n",
    "\n",
    "for s in corpus[:3]:\n",
    "    print(s)\n",
    "    print(simple_word_tokenize(s))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcd3e83",
   "metadata": {},
   "source": [
    "### Subword-токенизация: BPE / WordPiece / Unigram (12–14 мин)\n",
    "\n",
    "**Почему subword:**\n",
    "- решает проблему OOV (out-of-vocabulary)\n",
    "- хорошо работает на морфологически богатых языках\n",
    "- балансирует между “словом” и “символом”\n",
    "\n",
    "Коротко:\n",
    "- **BPE**: итеративно сливает самые частые пары токенов.\n",
    "- **WordPiece**: похож на BPE, но оптимизирует “правдоподобие” (используется в BERT).\n",
    "- **Unigram**: начинает с большого словаря подслов и удаляет худшие (SentencePiece).\n",
    "\n",
    "Ниже — обучим игрушечные токенизаторы на нашем корпусе и сравним разбиение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e153f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Попробуем использовать HuggingFace tokenizers. Если нет — дадим пояснение.\n",
    "try:\n",
    "    from tokenizers import Tokenizer\n",
    "    from tokenizers.models import BPE, WordPiece, Unigram\n",
    "    from tokenizers.trainers import BpeTrainer, WordPieceTrainer, UnigramTrainer\n",
    "    from tokenizers.pre_tokenizers import Whitespace\n",
    "    from tokenizers.normalizers import NFKC\n",
    "    TOKENIZERS_OK = True\n",
    "except Exception as e:\n",
    "    TOKENIZERS_OK = False\n",
    "    print(\"HuggingFace tokenizers not available:\", type(e).__name__, \"-\", e)\n",
    "\n",
    "TOKENIZERS_OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ab7762",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [basic_preprocess(t) for t in corpus]\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8518fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe(texts, vocab_size=80):\n",
    "    tok = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    tok.normalizer = NFKC()\n",
    "    tok.pre_tokenizer = Whitespace()\n",
    "    trainer = BpeTrainer(vocab_size=vocab_size, special_tokens=[\"[UNK]\",\"[PAD]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"])\n",
    "    tok.train_from_iterator(texts, trainer=trainer)\n",
    "    return tok\n",
    "\n",
    "def train_wordpiece(texts, vocab_size=80):\n",
    "    tok = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "    tok.normalizer = NFKC()\n",
    "    tok.pre_tokenizer = Whitespace()\n",
    "    trainer = WordPieceTrainer(vocab_size=vocab_size, special_tokens=[\"[UNK]\",\"[PAD]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"])\n",
    "    tok.train_from_iterator(texts, trainer=trainer)\n",
    "    return tok\n",
    "\n",
    "def train_unigram(texts, vocab_size=80):\n",
    "    tok = Tokenizer(Unigram())\n",
    "    tok.normalizer = NFKC()\n",
    "    tok.pre_tokenizer = Whitespace()\n",
    "    trainer = UnigramTrainer(vocab_size=vocab_size, special_tokens=[\"[UNK]\",\"[PAD]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"])\n",
    "    tok.train_from_iterator(texts, trainer=trainer)\n",
    "    return tok\n",
    "\n",
    "if TOKENIZERS_OK:\n",
    "    bpe_tok = train_bpe(texts, vocab_size=80)\n",
    "    wp_tok  = train_wordpiece(texts, vocab_size=80)\n",
    "    uni_tok = train_unigram(texts, vocab_size=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d321762",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"Кошки любят молоко, а кот любит рыбу. Tokenization is fun!\"\n",
    "sample = basic_preprocess(sample)\n",
    "\n",
    "if TOKENIZERS_OK:\n",
    "    print(\"SAMPLE:\", sample)\n",
    "    print(\"\\nBPE:\", bpe_tok.encode(sample).tokens)\n",
    "    print(\"\\nWordPiece:\", wp_tok.encode(sample).tokens)\n",
    "    print(\"\\nUnigram:\", uni_tok.encode(sample).tokens)\n",
    "else:\n",
    "    print(\"Skip: tokenizers library not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd87f261",
   "metadata": {},
   "source": [
    "**Что обсудить по результатам:**\n",
    "- где появляется много мелких кусочков?\n",
    "- как ведут себя русские окончания?\n",
    "- как отличаются “маркер продолжения слова” (в WordPiece часто `##...`) — зависит от реализации/словаря."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c916d9",
   "metadata": {},
   "source": [
    "## 4) Count-based вектора: Bag-of-Words и TF-IDF (8–10 мин)\n",
    "\n",
    "**Bag-of-Words (BoW)**: считаем, сколько раз токены встречаются в документе.  \n",
    "**TF-IDF**: downweight частые слова, upweight специфические.\n",
    "\n",
    "Плюсы:\n",
    "- быстро, просто, часто работает “удивительно хорошо” на классификации.\n",
    "\n",
    "Минусы:\n",
    "- теряется порядок слов (мешок слов)\n",
    "- огромная разреженность\n",
    "- слабая переносимость на новые домены\n",
    "- плохо с семантикой (синонимы, перефразирование)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf4fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "toy_docs = [\n",
    "    \"I love this movie, it is fantastic and fun\",\n",
    "    \"This film was terrible, boring and bad\",\n",
    "    \"What a great and wonderful experience\",\n",
    "    \"Awful plot, bad acting, I hate it\",\n",
    "    \"Fantastic acting and great story\",\n",
    "    \"Boring movie, not good\",\n",
    "]\n",
    "toy_y = np.array([1,0,1,0,1,0])  # 1=positive, 0=negative\n",
    "\n",
    "bow = CountVectorizer(lowercase=True)\n",
    "X_bow = bow.fit_transform(toy_docs)\n",
    "\n",
    "tfidf = TfidfVectorizer(lowercase=True)\n",
    "X_tfidf = tfidf.fit_transform(toy_docs)\n",
    "\n",
    "print(\"BoW shape:\", X_bow.shape, \"nnz:\", X_bow.nnz)\n",
    "print(\"TF-IDF shape:\", X_tfidf.shape, \"nnz:\", X_tfidf.nnz)\n",
    "print(\"\\nVocabulary:\", list(bow.vocabulary_.keys())[:10], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b47369f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим топ-слова по TF-IDF в одном документе\n",
    "doc_id = 0\n",
    "row = X_tfidf[doc_id].toarray().ravel()\n",
    "top = row.argsort()[::-1][:8]\n",
    "inv_vocab = {i:w for w,i in tfidf.vocabulary_.items()}\n",
    "[(inv_vocab[i], row[i]) for i in top if row[i] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f6305e",
   "metadata": {},
   "source": [
    "### Ограничения count-based подходов (что проговорить)\n",
    "\n",
    "1) **Нет порядка**: “dog bites man” vs “man bites dog” → одинаково (в BoW).  \n",
    "2) **Синонимы**: “great” и “excellent” — разные координаты.  \n",
    "3) **Разреженность**: словарь растёт, память/время растут.  \n",
    "4) **Доменные сдвиги**: новые слова/жанры → деградация.  \n",
    "5) **Морфология**: без лемматизации “кошки/кошку/кошкой” раздувают словарь."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff39feaf",
   "metadata": {},
   "source": [
    "## 5) Эмбеддинги: Word2Vec, GloVe, “плотные” представления (7–9 мин)\n",
    "\n",
    "**Эмбеддинг**: отображение токена в плотный вектор `R^d`, где близость ≈ семантическая близость.\n",
    "\n",
    "- **Word2Vec (CBOW/Skip-gram)**: предсказываем контекст по слову или слово по контексту.\n",
    "- **GloVe**: использует глобальные матрицы совместной встречаемости (co-occurrence) и факторизацию с весами.\n",
    "\n",
    "Здесь:\n",
    "- покажем идею на мини-примере\n",
    "- (опционально) обучим маленький Word2Vec, если доступен `gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8345ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Идея: если два слова часто встречаются в похожих контекстах — их вектора должны быть близки.\n",
    "\n",
    "contexts = [\n",
    "    (\"king\", [\"man\",\"royal\",\"crown\"]),\n",
    "    (\"queen\", [\"woman\",\"royal\",\"crown\"]),\n",
    "    (\"apple\", [\"fruit\",\"sweet\",\"tree\"]),\n",
    "    (\"orange\", [\"fruit\",\"sweet\",\"citrus\"]),\n",
    "]\n",
    "contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a02689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_train_word2vec(sentences):\n",
    "    try:\n",
    "        from gensim.models import Word2Vec\n",
    "        model = Word2Vec(sentences=sentences, vector_size=50, window=3, min_count=1, sg=1, epochs=200, workers=1)\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(\"gensim Word2Vec not available:\", type(e).__name__, \"-\", e)\n",
    "        return None\n",
    "\n",
    "# сделаем \"предложения\" из нашего мини-корпуса\n",
    "sentences = [simple_word_tokenize(t.lower()) for t in corpus]\n",
    "w2v = try_train_word2vec(sentences)\n",
    "\n",
    "if w2v is not None:\n",
    "    print(w2v.wv.most_similar(\"fun\", topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de8b753",
   "metadata": {},
   "source": [
    "**Обсуждение:**\n",
    "- На маленьком корпусе качества ждать не надо — цель увидеть *механику*.\n",
    "- На большом корпусе Word2Vec/GloVe дают полезные “геометрические” свойства.\n",
    "\n",
    "**Ограничения классических word embeddings:**\n",
    "- один вектор на слово → полисемия (“bank” как берег/банк)\n",
    "- контекст не учитывается (в отличие от BERT/LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5403aaa8",
   "metadata": {},
   "source": [
    "## 6) Базовая классификация текста (8–10 мин)\n",
    "\n",
    "Собираем простой pipeline:\n",
    "- текст → TF-IDF\n",
    "- классификатор: Logistic Regression / Linear SVM / Naive Bayes\n",
    "\n",
    "Это “рабочая лошадка” для задач:\n",
    "- sentiment\n",
    "- topic classification\n",
    "- spam detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878d377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(toy_docs, toy_y, test_size=0.33, random_state=42, stratify=toy_y)\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(lowercase=True, ngram_range=(1,2))),\n",
    "    (\"lr\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Test docs:\", X_test)\n",
    "print(\"Pred:\", pred, \"True:\", y_test)\n",
    "print()\n",
    "print(classification_report(y_test, pred, digits=3))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b642ee3e",
   "metadata": {},
   "source": [
    "### Интерпретация: какие признаки важны?\n",
    "\n",
    "Для линейных моделей на TF-IDF можно посмотреть веса по словам/нграммам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5639e4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = clf.named_steps[\"tfidf\"]\n",
    "lr = clf.named_steps[\"lr\"]\n",
    "\n",
    "feature_names = np.array(vec.get_feature_names_out())\n",
    "coef = lr.coef_[0]  # positive class\n",
    "top_pos = coef.argsort()[::-1][:10]\n",
    "top_neg = coef.argsort()[:10]\n",
    "\n",
    "print(\"Top positive features:\")\n",
    "for i in top_pos:\n",
    "    print(f\"{feature_names[i]:<20} {coef[i]:.3f}\")\n",
    "\n",
    "print(\"\\nTop negative features:\")\n",
    "for i in top_neg:\n",
    "    print(f\"{feature_names[i]:<20} {coef[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca7bd7",
   "metadata": {},
   "source": [
    "## 7) Итоги (1–2 мин)\n",
    "\n",
    "- Текст → токены → индексы → вектора → модель\n",
    "- Предобработка помогает, но может и вредить (зависит от задачи)\n",
    "- Subword-токенизация (BPE/WordPiece/Unigram) решает OOV и помогает морфологии\n",
    "- Count-based (BoW/TF-IDF) — сильный baseline, но без семантики и порядка\n",
    "- Эмбеддинги (Word2Vec/GloVe) дают “плотную” семантику, но контекст ограничен\n",
    "- Базовая классификация = TF-IDF + линейная модель — отличный стартовый baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fba6fa0",
   "metadata": {},
   "source": [
    "### Домашнее/практика (если останется 3–5 минут)\n",
    "\n",
    "1) Добавьте в TF-IDF `min_df` и `max_df`: как меняется словарь?  \n",
    "2) Попробуйте `MultinomialNB` и `LinearSVC` вместо логрегрессии.  \n",
    "3) Сравните токенизацию (word vs subword) для пары русских словоформ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-khabutdinov_sft]",
   "language": "python",
   "name": "conda-env-.conda-khabutdinov_sft-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
